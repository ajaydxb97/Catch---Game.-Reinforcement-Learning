{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "import seaborn\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "seaborn.set()\n",
    "\n",
    "class Catch(object):\n",
    "    \"\"\"\n",
    "    Class catch is the actual game.\n",
    "    \"\"\"\n",
    "    def __init__(self, grid_size=10):\n",
    "        self.grid_size = grid_size\n",
    "        self.reset()\n",
    "\n",
    "    def _update_state(self, action):\n",
    "        \"\"\"\n",
    "        Input: action and states\n",
    "        Ouput: new states and reward\n",
    "        \"\"\"\n",
    "        state = self.state\n",
    "        if action == 0:  # left\n",
    "            action = -1\n",
    "        elif action == 1:  # stay\n",
    "            action = 0\n",
    "        else:\n",
    "            action = 1  # right\n",
    "        f0, f1, basket = state[0]\n",
    "        new_basket = min(max(1, basket + action), self.grid_size-1)\n",
    "        f0 += 1\n",
    "        out = np.asarray([f0, f1, new_basket])\n",
    "        out = out[np.newaxis]\n",
    "\n",
    "        assert len(out.shape) == 2\n",
    "        self.state = out\n",
    "\n",
    "    def _draw_state(self):\n",
    "        im_size = (self.grid_size,)*2\n",
    "        state = self.state[0]\n",
    "        canvas = np.zeros(im_size)\n",
    "        canvas[state[0], state[1]] = 1  # draw fruit\n",
    "        canvas[-1, state[2]-1:state[2] + 2] = 1  # draw basket\n",
    "        return canvas\n",
    "        \n",
    "    def _get_reward(self):\n",
    "        fruit_row, fruit_col, basket = self.state[0]\n",
    "        if fruit_row == self.grid_size-1:\n",
    "            if abs(fruit_col - basket) <= 1:\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def _is_over(self):\n",
    "        if self.state[0, 0] == self.grid_size-1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self._draw_state()\n",
    "        return canvas.reshape((1, -1))\n",
    "\n",
    "    def act(self, action):\n",
    "        self._update_state(action)\n",
    "        reward = self._get_reward()\n",
    "        game_over = self._is_over()\n",
    "        return self.observe(), reward, game_over\n",
    "\n",
    "    def reset(self):\n",
    "        n = np.random.randint(0, self.grid_size-1, size=1)\n",
    "        m = np.random.randint(1, self.grid_size-2, size=1)\n",
    "        self.state = np.asarray([0, n, m])[np.newaxis]\n",
    "\n",
    "        \n",
    "last_frame_time = 0\n",
    "translate_action = [\"Left\",\"Stay\",\"Right\",\"Create Ball\",\"End Test\"]\n",
    "grid_size = 10\n",
    "\n",
    "def display_screen(action,points,input_t,counter):\n",
    "    global last_frame_time\n",
    "    print(\"Action %s, Points: %d\" % (translate_action[action],points))\n",
    "    #Only display the game screen if the game is not over\n",
    "    if(\"End\" not in translate_action[action]):\n",
    "        #Render the game with matplotlib\n",
    "        plt.imshow(input_t.reshape((grid_size,)*2), interpolation='none', cmap='gray')\n",
    "        plt.savefig('./frame'+str(counter)+'.png')\n",
    "        #Clear whatever we rendered before\n",
    "#         display.clear_output(wait=True)\n",
    "        #And display the rendering\n",
    "#         display.display(plt.gcf())\n",
    "    #Update the last frame time\n",
    "    last_frame_time = set_max_fps(last_frame_time)\n",
    "    \n",
    "    \n",
    "def set_max_fps(last_frame_time,FPS = 16):\n",
    "    current_milli_time = lambda: int(round(time.time() * 1000))\n",
    "    sleep_time = 1./FPS - (current_milli_time() - last_frame_time)\n",
    "    sleep_time = 1./FPS\n",
    "    if sleep_time > 0:\n",
    "        time.sleep(sleep_time)\n",
    "    return current_milli_time()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import random\n",
    "\n",
    "import os\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential,load_model, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Multiply\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, Lambda\n",
    "from keras.optimizers import Adam, Adamax, RMSprop\n",
    "from keras import backend as K\n",
    "\n",
    "MAX_TIMESTEP = 1000\n",
    "MAX_EP = 100000\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.1\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "random.seed(2)\n",
    "np.random.seed(2)\n",
    "tf.set_random_seed(2)\n",
    "\n",
    "def categorical_crossentropy(target, output):\n",
    "    _epsilon =  tf.convert_to_tensor(10e-8, dtype=output.dtype.base_dtype)\n",
    "    output = tf.clip_by_value(output, _epsilon, 1. - _epsilon)\n",
    "    return (- target * tf.log(output))\n",
    "\n",
    "\n",
    "class Agent_ActorCritic():\n",
    "    def __init__(self, env):\n",
    "        self.log_path = './actor_critic.log'\n",
    "\n",
    "        self.env = env\n",
    "        self.actions_avialbe = 3\n",
    "        self.feature_dim = 100\n",
    "        self.t = 0\n",
    "        self.prev_x = None\n",
    "        self.actor_learning_rate  = 1e-3\n",
    "        self.critic_learning_rate = 1e-3\n",
    "        self.gamma = 0.9\n",
    "        self.dummy_act_picked = np.zeros((1,self.actions_avialbe))\n",
    "        self.episode_number_list = []\n",
    "        self.average_reward_list = []\n",
    "        self.entropy_list = []\n",
    "        \n",
    "        # Actor\n",
    "        input_frame  = Input(shape=(self.feature_dim,))\n",
    "        act_picked = Input(shape=(self.actions_avialbe,))\n",
    "        hidden_f = Dense(20,activation='relu')(input_frame)\n",
    "\n",
    "        act_prob = Dense(self.actions_avialbe,activation='softmax')(hidden_f)\n",
    "        selected_act_prob = Multiply()([act_prob,act_picked])\n",
    "        selected_act_prob = Lambda(lambda x:K.sum(x, axis=-1, keepdims=True),output_shape=(1,))(selected_act_prob)\n",
    "\n",
    "        model = Model(inputs=[input_frame,act_picked], outputs=[act_prob, selected_act_prob])\n",
    "\n",
    "        opt = Adam(lr=self.actor_learning_rate)\n",
    "        model.compile(loss=['mse',categorical_crossentropy], loss_weights=[0.0,1.0],optimizer=opt)\n",
    "        self.actor = model\n",
    "\n",
    "        # Critic\n",
    "        model = Sequential()\n",
    "        model.add(Dense(20,activation='relu',input_shape=(self.feature_dim,)))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        opt = Adam(lr=self.critic_learning_rate)\n",
    "        model.compile(loss='mse', optimizer=opt)\n",
    "        self.critic = model\n",
    "\n",
    "    def init_game_setting(self):\n",
    "        self.prev_x = None\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        # Init\n",
    "        log = open(self.log_path,'w')\n",
    "        batch_size = 1 \n",
    "        frames, prob_actions, dlogps, drs =[], [], [], []\n",
    "        tr_x, tr_y = [],[]\n",
    "        reward_record = []\n",
    "        avg_reward = []\n",
    "        avg_entropy = []\n",
    "        reward_sum = 0\n",
    "        ep_number = 0\n",
    "        ep_step = 0 \n",
    "        self.env.reset()\n",
    "        observation = self.env.observe()\n",
    "        maxs = 0\n",
    "        entropy_per_episode = 0\n",
    "        while True:\n",
    "            \n",
    "#             temp_p = self.actor.predict([observation,self.dummy_act_picked])[0].flatten()\n",
    "            temp_p = self.actor.predict([observation,self.dummy_act_picked])\n",
    "            \n",
    "            entropy = -np.sum(temp_p[0][0] * np.log(temp_p[0][0]))\n",
    "            entropy_per_episode = entropy_per_episode + entropy\n",
    "            act = np.random.choice(np.arange(self.actions_avialbe), \n",
    "                    p=temp_p[0][0])\n",
    "\n",
    "            act_one_hot = np.zeros((1,self.actions_avialbe))\n",
    "            act_one_hot[0,act]=1.0\n",
    "            next_observation, reward, done = self.env.act(act)\n",
    "            #if done: reward = -20\n",
    "            \n",
    "            reward_sum += reward\n",
    "            predict_reward = self.critic.predict(observation)\n",
    "            predict_next_reward = self.critic.predict(next_observation)\n",
    "\n",
    "            td_target = np.expand_dims(reward,axis=0) + self.gamma*predict_next_reward \n",
    "            td_error = td_target - predict_reward + 0.01*entropy\n",
    "\n",
    "            self.critic.train_on_batch(observation,td_target)\n",
    "            self.actor.train_on_batch([observation,act_one_hot],[self.dummy_act_picked,td_error])\n",
    "\n",
    "            observation = next_observation\n",
    "\n",
    "            self.t += 1\n",
    "            ep_step += 1\n",
    "\n",
    "            if done or ep_step>MAX_TIMESTEP:\n",
    "                ep_number += 1\n",
    "                entropy_per_episode = entropy_per_episode/ep_step\n",
    "                avg_entropy.append(entropy_per_episode)\n",
    "                avg_reward.append(float(reward_sum))\n",
    "                if len(avg_reward)>300: avg_reward.pop(0)\n",
    "                if len(avg_entropy)>300: avg_entropy.pop(0)\n",
    "                mean_entropy = np.mean(avg_entropy)\n",
    "                mean_reward = np.mean(avg_reward)\n",
    "                self.entropy_list.append(mean_entropy)\n",
    "                print('EPISODE: {0:6d} / TIMESTEP: {1:8d} / REWARD: {2:5d} / AVG_REWARD: {3:2.3f} '.format(\n",
    "                            ep_number, self.t, int(reward_sum), mean_reward),mean_entropy)\n",
    "#                 print('{:.4f},{:.4f}'.format(reward_sum,np.mean(avg_reward)),end='\\n',file=log,flush=True)\n",
    "                entropy_per_episode = 0\n",
    "                self.env.reset()\n",
    "                observation = self.env.observe()\n",
    "                reward_sum = 0.0\n",
    "                ep_step = 0\n",
    "                self.episode_number_list.append(ep_number)\n",
    "                self.average_reward_list.append(mean_reward)\n",
    "                log.write(str(ep_number)+\" \"+str(mean_reward))\n",
    "                if (maxs < mean_reward or mean_reward==1)  and (ep_number > 300):\n",
    "                    maxs = mean_reward\n",
    "                    print(\"saved model maxs:\", maxs, \"caught fruits (300): \",(300*maxs+300)/2)\n",
    "                    if (maxs >= 0.90 or (maxs<0.6 and maxs > 0.58) or (maxs<0.2 and maxs > 0.18)):\n",
    "                        name_actor = \"actor\"+str(maxs)+\".h5\"\n",
    "                        name_critic = \"critic\"+str(maxs)+\".h5\"\n",
    "                        self.actor.save(name_actor)\n",
    "                        self.critic.save(name_critic)\n",
    "                        if (maxs>= 0.90):\n",
    "                            plt.figure(1)\n",
    "                            plt.plot(self.episode_number_list,self.average_reward_list)\n",
    "                            plt.xlabel('episodes')\n",
    "                            plt.ylabel('average reward')\n",
    "                            plt.savefig('./learning_curve.png')\n",
    "                            plt.show()\n",
    "                            plt.figure(2)\n",
    "                            plt.plot(self.episode_number_list,self.entropy_list)\n",
    "                            plt.xlabel('episodes')\n",
    "                            plt.ylabel('entropy')\n",
    "                            plt.savefig('./entropy_curve.png')\n",
    "                            plt.show()\n",
    "                            break\n",
    "                    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE:      1 / TIMESTEP:        9 / REWARD:    -1 / AVG_REWARD: -1.000  1.09173760149214\n",
      "EPISODE:      2 / TIMESTEP:       18 / REWARD:    -1 / AVG_REWARD: -1.000  1.090909163157145\n",
      "EPISODE:      3 / TIMESTEP:       27 / REWARD:    -1 / AVG_REWARD: -1.000  1.0914982733903107\n",
      "EPISODE:      4 / TIMESTEP:       36 / REWARD:    -1 / AVG_REWARD: -1.000  1.0893589655558267\n",
      "EPISODE:      5 / TIMESTEP:       45 / REWARD:    -1 / AVG_REWARD: -1.000  1.0887049595514933\n",
      "EPISODE:      6 / TIMESTEP:       54 / REWARD:    -1 / AVG_REWARD: -1.000  1.0867860361381814\n",
      "EPISODE:      7 / TIMESTEP:       63 / REWARD:    -1 / AVG_REWARD: -1.000  1.0871080576427399\n",
      "EPISODE:      8 / TIMESTEP:       72 / REWARD:     1 / AVG_REWARD: -0.750  1.0865150690078735\n",
      "EPISODE:      9 / TIMESTEP:       81 / REWARD:    -1 / AVG_REWARD: -0.778  1.0854238360016435\n",
      "EPISODE:     10 / TIMESTEP:       90 / REWARD:    -1 / AVG_REWARD: -0.800  1.0863422526253594\n",
      "EPISODE:     11 / TIMESTEP:       99 / REWARD:    -1 / AVG_REWARD: -0.818  1.0866630631263812\n",
      "EPISODE:     12 / TIMESTEP:      108 / REWARD:    -1 / AVG_REWARD: -0.833  1.0870236533659476\n",
      "EPISODE:     13 / TIMESTEP:      117 / REWARD:    -1 / AVG_REWARD: -0.846  1.0864296142871566\n",
      "EPISODE:     14 / TIMESTEP:      126 / REWARD:     1 / AVG_REWARD: -0.714  1.0863681870793542\n",
      "EPISODE:     15 / TIMESTEP:      135 / REWARD:     1 / AVG_REWARD: -0.600  1.085992811344288\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-30be161d9a8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent_ActorCritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# plt.figure(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# plt.plot(agent.episode_number_list,agent.average_reward_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# plt.xlabel('episodes')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-a735fc76d1b7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m#             temp_p = self.actor.predict([observation,self.dummy_act_picked])[0].flatten()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mtemp_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdummy_act_picked\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;31m#             print(temp_p)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = Agent_ActorCritic(Catch(10))\n",
    "agent.train()\n",
    "# plt.figure(1)\n",
    "# plt.plot(agent.episode_number_list,agent.average_reward_list)\n",
    "# plt.xlabel('episodes')\n",
    "# plt.ylabel('average reward')\n",
    "# plt.savefig('./learning_curve.png')\n",
    "# plt.show()\n",
    "# plt.figure(2)\n",
    "# plt.plot(agent.episode_number_list,agent.entropy_list)\n",
    "# plt.xlabel('episodes')\n",
    "# plt.ylabel('entropy')\n",
    "# plt.savefig('./entropy_curve.png')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Left, Points: 0\n",
      "Action Left, Points: 0\n",
      "Action Left, Points: 0\n",
      "Action Left, Points: 0\n",
      "Action Stay, Points: 0\n",
      "Action Left, Points: 0\n",
      "Action Right, Points: 0\n",
      "Action Stay, Points: 0\n",
      "Action Stay, Points: -1\n",
      "Action Left, Points: -1\n",
      "Action Stay, Points: -1\n",
      "Action Stay, Points: -1\n",
      "Action Stay, Points: -1\n",
      "Action Stay, Points: -1\n",
      "Action Stay, Points: -1\n",
      "Action Stay, Points: -1\n",
      "Action Stay, Points: -1\n",
      "Action Stay, Points: -2\n",
      "Action Left, Points: -2\n",
      "Action Left, Points: -2\n",
      "Action Right, Points: -2\n",
      "Action Right, Points: -2\n",
      "Action Stay, Points: -2\n",
      "Action Stay, Points: -2\n",
      "Action Stay, Points: -2\n",
      "Action Stay, Points: -2\n",
      "Action Stay, Points: -3\n",
      "Action Stay, Points: -3\n",
      "Action Stay, Points: -3\n",
      "Action Stay, Points: -3\n",
      "Action Stay, Points: -3\n",
      "Action Stay, Points: -3\n",
      "Action Stay, Points: -3\n",
      "Action Stay, Points: -3\n",
      "Action Stay, Points: -3\n",
      "Action Stay, Points: -2\n",
      "Action Left, Points: -2\n",
      "Action Left, Points: -2\n",
      "Action Left, Points: -2\n",
      "Action Left, Points: -2\n",
      "Action Left, Points: -2\n",
      "Action Left, Points: -2\n",
      "Action Left, Points: -2\n",
      "Action Left, Points: -2\n",
      "Action Left, Points: -1\n",
      "Action Right, Points: -1\n",
      "Action Stay, Points: -1\n",
      "Action Stay, Points: -1\n",
      "Action Stay, Points: -1\n",
      "Action Stay, Points: -1\n",
      "Action Stay, Points: -1\n",
      "Action Stay, Points: -1\n",
      "Action Stay, Points: -1\n",
      "Action Stay, Points: 0\n",
      "Action Right, Points: 0\n",
      "Action Stay, Points: 0\n",
      "Action Left, Points: 0\n",
      "Action Left, Points: 0\n",
      "Action Left, Points: 0\n",
      "Action Left, Points: 0\n",
      "Action Left, Points: 0\n",
      "Action Left, Points: 0\n",
      "Action Left, Points: 1\n",
      "Action Stay, Points: 1\n",
      "Action Stay, Points: 1\n",
      "Action Stay, Points: 1\n",
      "Action Left, Points: 1\n",
      "Action Stay, Points: 1\n",
      "Action Left, Points: 1\n",
      "Action Right, Points: 1\n",
      "Action Stay, Points: 1\n",
      "Action Stay, Points: 0\n",
      "Action Stay, Points: 0\n",
      "Action Right, Points: 0\n",
      "Action Right, Points: 0\n",
      "Action Stay, Points: 0\n",
      "Action Right, Points: 0\n",
      "Action Left, Points: 0\n",
      "Action Right, Points: 0\n",
      "Action Stay, Points: 0\n",
      "Action Left, Points: 1\n",
      "Action Right, Points: 1\n",
      "Action Right, Points: 1\n",
      "Action Stay, Points: 1\n",
      "Action Stay, Points: 1\n",
      "Action Stay, Points: 1\n",
      "Action Stay, Points: 1\n",
      "Action Stay, Points: 1\n",
      "Action Stay, Points: 1\n",
      "Action Stay, Points: 2\n",
      "Action Stay, Points: 2\n",
      "Action Right, Points: 2\n",
      "Action Right, Points: 2\n",
      "Action Stay, Points: 2\n",
      "Action Right, Points: 2\n",
      "Action Left, Points: 2\n",
      "Action Right, Points: 2\n",
      "Action Stay, Points: 2\n",
      "Action Left, Points: 3\n",
      "Action Left, Points: 3\n",
      "Action Stay, Points: 3\n",
      "Action Right, Points: 3\n",
      "Action Left, Points: 3\n",
      "Action Right, Points: 3\n",
      "Action Left, Points: 3\n",
      "Action Right, Points: 3\n",
      "Action Stay, Points: 3\n",
      "Action Left, Points: 4\n",
      "Action Right, Points: 4\n",
      "Action Left, Points: 4\n",
      "Action Left, Points: 4\n",
      "Action Left, Points: 4\n",
      "Action Left, Points: 4\n",
      "Action Left, Points: 4\n",
      "Action Left, Points: 4\n",
      "Action Left, Points: 4\n",
      "Action Left, Points: 5\n",
      "Action Stay, Points: 5\n",
      "Action Stay, Points: 5\n",
      "Action Stay, Points: 5\n",
      "Action Stay, Points: 5\n",
      "Action Stay, Points: 5\n",
      "Action Left, Points: 5\n",
      "Action Right, Points: 5\n",
      "Action Stay, Points: 5\n",
      "Action Stay, Points: 6\n",
      "Action Stay, Points: 6\n",
      "Action Stay, Points: 6\n",
      "Action Right, Points: 6\n",
      "Action Stay, Points: 6\n",
      "Action Right, Points: 6\n",
      "Action Left, Points: 6\n",
      "Action Right, Points: 6\n",
      "Action Stay, Points: 6\n",
      "Action Left, Points: 7\n",
      "Action Right, Points: 7\n",
      "Action Stay, Points: 7\n",
      "Action Stay, Points: 7\n",
      "Action Left, Points: 7\n",
      "Action Right, Points: 7\n",
      "Action Left, Points: 7\n",
      "Action Right, Points: 7\n",
      "Action Stay, Points: 7\n",
      "Action Stay, Points: 6\n",
      "Action Left, Points: 6\n",
      "Action Left, Points: 6\n",
      "Action Stay, Points: 6\n",
      "Action Left, Points: 6\n",
      "Action Stay, Points: 6\n",
      "Action Left, Points: 6\n",
      "Action Right, Points: 6\n",
      "Action Stay, Points: 6\n",
      "Action Stay, Points: 5\n",
      "Action Right, Points: 5\n",
      "Action Stay, Points: 5\n",
      "Action Stay, Points: 5\n",
      "Action Stay, Points: 5\n",
      "Action Stay, Points: 5\n",
      "Action Left, Points: 5\n",
      "Action Right, Points: 5\n",
      "Action Stay, Points: 5\n",
      "Action Stay, Points: 6\n",
      "Action Left, Points: 6\n",
      "Action Left, Points: 6\n",
      "Action Right, Points: 6\n",
      "Action Left, Points: 6\n",
      "Action Right, Points: 6\n",
      "Action Left, Points: 6\n",
      "Action Right, Points: 6\n",
      "Action Stay, Points: 6\n",
      "Action Left, Points: 7\n",
      "Action Right, Points: 7\n",
      "Action Right, Points: 7\n",
      "Action Left, Points: 7\n",
      "Action Left, Points: 7\n",
      "Action Left, Points: 7\n",
      "Action Left, Points: 7\n",
      "Action Left, Points: 7\n",
      "Action Left, Points: 7\n",
      "Action Left, Points: 8\n",
      "Action Stay, Points: 8\n",
      "Action Stay, Points: 8\n",
      "Action Stay, Points: 8\n",
      "Action Left, Points: 8\n",
      "Action Stay, Points: 8\n",
      "Action Left, Points: 8\n",
      "Action Right, Points: 8\n",
      "Action Stay, Points: 8\n",
      "Action Stay, Points: 7\n",
      "Action Right, Points: 7\n",
      "Action Stay, Points: 7\n",
      "Action Stay, Points: 7\n",
      "Action Left, Points: 7\n",
      "Action Right, Points: 7\n",
      "Action Left, Points: 7\n",
      "Action Right, Points: 7\n",
      "Action Stay, Points: 7\n",
      "Action Stay, Points: 6\n",
      "Action Left, Points: 6\n",
      "Action Left, Points: 6\n",
      "Action Right, Points: 6\n",
      "Action Left, Points: 6\n",
      "Action Right, Points: 6\n",
      "Action Left, Points: 6\n",
      "Action Right, Points: 6\n",
      "Action Stay, Points: 6\n",
      "Action Left, Points: 7\n",
      "Action Left, Points: 7\n",
      "Action Left, Points: 7\n",
      "Action Stay, Points: 7\n",
      "Action Left, Points: 7\n",
      "Action Stay, Points: 7\n",
      "Action Left, Points: 7\n",
      "Action Stay, Points: 7\n",
      "Action Stay, Points: 7\n",
      "Action Stay, Points: 8\n",
      "Action Right, Points: 8\n",
      "Action Stay, Points: 8\n",
      "Action Stay, Points: 8\n",
      "Action Stay, Points: 8\n",
      "Action Stay, Points: 8\n",
      "Action Stay, Points: 8\n",
      "Action Stay, Points: 8\n",
      "Action Stay, Points: 8\n",
      "Action Stay, Points: 9\n",
      "Action Stay, Points: 9\n",
      "Action Stay, Points: 9\n",
      "Action Stay, Points: 9\n",
      "Action Left, Points: 9\n",
      "Action Stay, Points: 9\n",
      "Action Left, Points: 9\n",
      "Action Right, Points: 9\n",
      "Action Stay, Points: 9\n",
      "Action Stay, Points: 8\n",
      "Action Left, Points: 8\n",
      "Action Left, Points: 8\n",
      "Action Stay, Points: 8\n",
      "Action Stay, Points: 8\n",
      "Action Stay, Points: 8\n",
      "Action Stay, Points: 8\n",
      "Action Stay, Points: 8\n",
      "Action Stay, Points: 8\n",
      "Action Stay, Points: 9\n",
      "Action Stay, Points: 9\n",
      "Action Stay, Points: 9\n",
      "Action Left, Points: 9\n",
      "Action Left, Points: 9\n",
      "Action Left, Points: 9\n",
      "Action Left, Points: 9\n",
      "Action Left, Points: 9\n",
      "Action Left, Points: 9\n",
      "Action Left, Points: 10\n",
      "Action Left, Points: 10\n",
      "Action Left, Points: 10\n",
      "Action Stay, Points: 10\n",
      "Action Left, Points: 10\n",
      "Action Left, Points: 10\n",
      "Action Left, Points: 10\n",
      "Action Left, Points: 10\n",
      "Action Left, Points: 10\n",
      "Action Left, Points: 11\n",
      "Action Stay, Points: 11\n",
      "Action Left, Points: 11\n",
      "Action Stay, Points: 11\n",
      "Action Left, Points: 11\n",
      "Action Stay, Points: 11\n",
      "Action Left, Points: 11\n",
      "Action Right, Points: 11\n",
      "Action Stay, Points: 11\n",
      "Action Stay, Points: 10\n",
      "average value fn: [[0.07956173]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "env = Catch(10)\n",
    "actor = load_model('actor.h5')\n",
    "# critic =  load_model('critictor.h5')\n",
    "score = 0\n",
    "average_value_fn = 0\n",
    "counter = 0\n",
    "\n",
    "for e in range(30):\n",
    "    done = False\n",
    "    env.reset()\n",
    "    state = env.observe()\n",
    "    state = np.reshape(state, [1, 100])\n",
    "#     print(str(e)+\" \"+str(score))\n",
    "    dummy_act_picked=np.zeros((1,3))\n",
    "    while not done:\n",
    "\n",
    "        counter = counter + 1\n",
    "        temp_p = actor.predict([state,dummy_act_picked])\n",
    "        act = np.argmax(temp_p[0][0])\n",
    "#         print(act_one_hot)\n",
    "        act_one_hot = np.zeros((1,3))\n",
    "        act_one_hot[0,act]=1.0\n",
    "        dummy_act_picked = act_one_hot\n",
    "        next_state, reward, done = env.act(act)\n",
    "        next_state = np.reshape(next_state, [1, 100])\n",
    "        average_value_fn = average_value_fn + critic.predict(state) \n",
    "        score = score + reward\n",
    "        display_screen(act,score,next_state, counter)\n",
    "#         print(state.reshape(10,10))\n",
    "        #plt.imshow(next_state.reshape(10,10))\n",
    "    \n",
    "        #Clear whatever we rendered before\n",
    "        #display.clear_output(wait=True)\n",
    "        #display.display(plt.gcf())\n",
    "#         plt.show()\n",
    "        state = next_state\n",
    "#plt.show()        \n",
    "print('average value fn:', average_value_fn/(5000))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
